{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# for loading MNIST data\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0f8cabb410>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if cuda device is available then run model on gpu\n",
    "if torch.cuda.is_available():\n",
    "    cuda_flag=True\n",
    "else:\n",
    "    cuda_flag=False\n",
    "    \n",
    "torch.manual_seed(3120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                datasets.MNIST('./dataset/', train=True, download=True,transform=transform),\n",
    "                batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                datasets.MNIST('./dataset/', train=False, download=True, transform=transform),\n",
    "                 batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class definition for encoder decoder networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(encoder, self).__init__()        \n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=encoding_dim*4)\n",
    "        self.fc2 = nn.Linear(in_features=encoding_dim*4, out_features=encoding_dim*2)\n",
    "        self.fc3 = nn.Linear(in_features=encoding_dim*2, out_features=encoding_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self, output_dim, encoding_dim):\n",
    "        super(decoder, self).__init__()        \n",
    "        self.fc1 = nn.Linear(in_features=encoding_dim, out_features=encoding_dim*2)\n",
    "        self.fc2 = nn.Linear(in_features=encoding_dim*2, out_features=encoding_dim*4)\n",
    "        self.fc3 = nn.Linear(in_features=encoding_dim*4, out_features=output_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the models\n",
    "enc = encoder(input_dim=784, encoding_dim=100)\n",
    "dec = decoder(output_dim=784, encoding_dim=100)\n",
    "\n",
    "if cuda_flag:\n",
    "    enc,dec = enc.cuda(), dec.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer to update the model parameters for both encoder and decoder\n",
    "opt = optim.Adam(list(enc.parameters())+list(dec.parameters()), lr=1e-4)\n",
    "\n",
    "# define loss function\n",
    "crit = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a00efc33fca42c4a40356e4c77234f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\t Train Loss:0.22455\t\n",
      "Epoch:1\t Train Loss:0.103036\t\n",
      "Epoch:2\t Train Loss:0.0753213\t\n",
      "Epoch:3\t Train Loss:0.0630564\t\n",
      "Epoch:4\t Train Loss:0.0562923\t\n",
      "Epoch:5\t Train Loss:0.0510956\t\n",
      "Epoch:6\t Train Loss:0.0466395\t\n",
      "Epoch:7\t Train Loss:0.0432341\t\n",
      "Epoch:8\t Train Loss:0.0402964\t\n",
      "Epoch:9\t Train Loss:0.0379133\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss=0\n",
    "    \n",
    "    # iterate over dataset\n",
    "    for x,_ in train_loader:\n",
    "        x = x.view(len(x),-1)\n",
    "        # move data to gpu if cuda_flag is set\n",
    "        if cuda_flag:\n",
    "            x = x.cuda()\n",
    "        \n",
    "        # zero_grad to ensure no unaccounted calculation creeps in while calculating gradients\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # forward propogation and loss computation\n",
    "        x_gen = dec(enc(x))\n",
    "        loss = crit(x_gen,x)\n",
    "        train_loss+=loss.item()\n",
    "        \n",
    "        # backpropogate gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights\n",
    "        opt.step()\n",
    "    train_loss/=len(train_loader)\n",
    "\n",
    "    print (\"Epoch:{}\\t Train Loss:{:.6}\\t\".format(epoch,train_loss))\n",
    "    loss_history.append(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on test set:  0.03588985968499806\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for x,_ in test_loader:\n",
    "        x = x.view(len(x),-1)\n",
    "        if cuda_flag:\n",
    "            x = x.cuda()\n",
    "        \n",
    "        x_gen = dec(enc(x))\n",
    "        loss = crit(x_gen,x)\n",
    "        test_loss+=loss.item()\n",
    "test_loss/=len(test_loader)\n",
    "print(\"Loss on test set: \", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How good are our low dimensional embeddings?\n",
    "\n",
    "Let's see what good our low dimensional embeddings are at downstream classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building compressed dataset with encoder\n",
    "\n",
    "compressed_train_data = []\n",
    "train_labels = []\n",
    "with torch.no_grad():\n",
    "    for x,y in train_loader:\n",
    "        x = x.view(len(x),-1)\n",
    "        if cuda_flag:\n",
    "            x = x.cuda()\n",
    "\n",
    "        x_comp = enc(x).cpu()\n",
    "        compressed_train_data.append(x_comp)\n",
    "        train_labels.append(y)\n",
    "\n",
    "    compressed_train_data = torch.cat(compressed_train_data)\n",
    "    train_labels = torch.cat(train_labels)\n",
    "    compressed_train_loader = torch.utils.data.DataLoader(\n",
    "                                torch.utils.data.TensorDataset(compressed_train_data, train_labels),\n",
    "                                batch_size=batch_size, shuffle=True)\n",
    "\n",
    "compressed_test_data = []\n",
    "test_labels = []\n",
    "with torch.no_grad():\n",
    "    for x,y in test_loader:\n",
    "        x = x.view(len(x),-1)\n",
    "        if cuda_flag:\n",
    "            x = x.cuda()\n",
    "\n",
    "        x_comp = enc(x).cpu()\n",
    "        compressed_test_data.append(x_comp)\n",
    "        test_labels.append(y)\n",
    "\n",
    "    compressed_test_data = torch.cat(compressed_test_data)\n",
    "    test_labels = torch.cat(test_labels)\n",
    "    compressed_test_loader = torch.utils.data.DataLoader(\n",
    "                                torch.utils.data.TensorDataset(compressed_test_data, test_labels),\n",
    "                                batch_size=batch_size, shuffle=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(classifier, self).__init__()        \n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=input_dim//2)\n",
    "        self.fc2 = nn.Linear(in_features=self.fc1.out_features, out_features=self.fc1.out_features//2)\n",
    "        self.fc3 = nn.Linear(in_features=self.fc2.out_features, out_features=num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.softmax(self.fc3(x),1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, train_loader, test_loader, train_data_fraction=1):\n",
    "    if cuda_flag:\n",
    "        model = model.cuda()\n",
    "\n",
    "    opt = optim.Adam(model.parameters())\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    epochs = 10\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        train_loss=0\n",
    "        accuracy = 0\n",
    "\n",
    "        for i,(x,y) in enumerate(train_loader):\n",
    "            if ((i+1)/len(train_loader)>=train_data_fraction):\n",
    "                break\n",
    "            opt.zero_grad()\n",
    "\n",
    "            x = x.view(len(x),-1)\n",
    "            if cuda_flag:\n",
    "                x,y = x.cuda(), y.cuda()\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = crit(y_pred,y)\n",
    "            train_loss+=loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            _, predicted = torch.max(y_pred, 1)\n",
    "            accuracy += (predicted == y).sum().item()\n",
    "\n",
    "        train_loss/=len(train_loader)\n",
    "        train_accuracy = accuracy*100/(batch_size*len(train_loader)*train_data_fraction)\n",
    "        print (\"Epoch:{}\\t Train Loss:{:.6}\\t Train Accuracy:{:.4}\".format(epoch,train_loss,train_accuracy))\n",
    "\n",
    "    model.eval()\n",
    "    accuracy = 0\n",
    "    num_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in (test_loader):\n",
    "            x = x.view(len(x),-1)\n",
    "            if cuda_flag:\n",
    "                x,y = x.cuda(), y.cuda()\n",
    "            _, predicted = torch.max(model(x).detach(), 1) # detach to avoid accidental gradient backpropogation\n",
    "            num_examples+=len(predicted)\n",
    "            correct = (predicted == y).sum()\n",
    "            accuracy += correct.item() \n",
    "\n",
    "    print (\"Test Accuracy:\",accuracy/num_examples)\n",
    "    return train_accuracy, accuracy/num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\t Train Loss:1.61197\t Train Accuracy:85.4\n",
      "Epoch:1\t Train Loss:1.53135\t Train Accuracy:92.89\n",
      "Epoch:2\t Train Loss:1.51854\t Train Accuracy:94.09\n",
      "Epoch:3\t Train Loss:1.51208\t Train Accuracy:94.69\n",
      "Epoch:4\t Train Loss:1.50588\t Train Accuracy:95.27\n",
      "Epoch:5\t Train Loss:1.5036\t Train Accuracy:95.5\n",
      "Epoch:6\t Train Loss:1.5006\t Train Accuracy:95.8\n",
      "Epoch:7\t Train Loss:1.4989\t Train Accuracy:95.96\n",
      "Epoch:8\t Train Loss:1.49608\t Train Accuracy:96.23\n",
      "Epoch:9\t Train Loss:1.49507\t Train Accuracy:96.35\n",
      "Test Accuracy: 0.9661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(96.35027985074628, 0.9661)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = classifier(784,10)\n",
    "trainer(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on Encoded Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\t Train Loss:1.75749\t Train Accuracy:72.81\n",
      "Epoch:1\t Train Loss:1.62884\t Train Accuracy:83.57\n",
      "Epoch:2\t Train Loss:1.6031\t Train Accuracy:86.1\n",
      "Epoch:3\t Train Loss:1.54172\t Train Accuracy:92.4\n",
      "Epoch:4\t Train Loss:1.52721\t Train Accuracy:93.65\n",
      "Epoch:5\t Train Loss:1.51895\t Train Accuracy:94.42\n",
      "Epoch:6\t Train Loss:1.51313\t Train Accuracy:94.94\n",
      "Epoch:7\t Train Loss:1.5083\t Train Accuracy:95.39\n",
      "Epoch:8\t Train Loss:1.50498\t Train Accuracy:95.69\n",
      "Epoch:9\t Train Loss:1.50175\t Train Accuracy:95.98\n",
      "Test Accuracy: 0.9583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(95.98380863539445, 0.9583)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = classifier(100,10)\n",
    "trainer(model, compressed_train_loader, compressed_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the above training and test accuracies, the model trained on lower dimensional encoding dataset has __test accuracy at par__ with model trained on original dataset despite having significantly __less parameters__. Another advantage of dimension reduction is the reduced requirement of labelled data owing to less parameters in the supervised task model, thus improving data efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
